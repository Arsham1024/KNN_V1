# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z1_Hqt-NXP04WzIs9I_vF5AIxuZsnMO0

Using colab is another option if you guys want:)
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np              # linear algebra
import pandas as pd             # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # for plotting and visualozing data

import pandas as pd             #importing pandas for better visuals

#Import the data set
fruits = pd.read_table('fruit_data_with_colors.txt')

#Check the data set. Showing the first 5 elements
fruits.head()

# create a mapping from fruit label value to fruit name to make results easier to interpret
# using dictionaries in python. Notice that each fruit is associated with the label now
predct = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   
predct

# checking how many unique fruit names are present in the dataset
# 
fruits['fruit_name'].value_counts()

#categroizing the data into all the types of fruit it has. in this data set we have four types of fruit.

apple_data    = fruits [fruits['fruit_name'] == 'apple']
orange_data   = fruits [fruits['fruit_name'] == 'orange']
lemon_data    = fruits [fruits['fruit_name'] == 'lemon']
mandarin_data = fruits [fruits['fruit_name'] == 'mandarin']

#Checking the data stored in each fruit array
#For each fruit the label is different
# For apple it is 1
apple_data.head()

#Plotting the entire data set in a graph. This is for visualization purposes.
plt.scatter(fruits['width'],fruits['height'])

plt.scatter(fruits['mass'],fruits['height'])

import math

#  This is the distance method
#  given two data points, calculate the euclidean distance between them


def get_distance(data1, data2):

    points = zip(data1, data2)
    diffs_squared_distance = [pow(a - b, 2) for (a, b) in points]
    return math.sqrt(sum(diffs_squared_distance))

# calculate the Euclidean distance between two vectors
from math import sqrt
def get_distance2(data1, data2):
	distance = 0.0
	for i in range(len(data1)-1):
		distance += (data1[i] - data2[i])**2
	return sqrt(distance)

# Test distance function
# dataset = [[test2[0][0],test2[0][1],test2[0][2]],
# 	[1.465489372,2.362125076,0],
# 	[3.396561688,4.400293529,0],
# 	[1.38807019,1.850220317,0],
# 	[3.06407232,3.005305973,0],
# 	[7.627531214,2.759262235,1],
# 	[5.332441248,2.088626775,1],
# 	[6.922596716,1.77106367,1],
# 	[8.675418651,-0.242068655,1],
# 	[7.673756466,3.508563011,1]]

# for i in range(1, len(dataset)-1):


# Initializing the dataset array to the same size array as the train array
dataset = np.empty(len(train2), dtype=object)

#putting the first test case into the first spot of the array
dataset[0] = test2[0]

# giving the rest of the data set values the same values as the train array
for i in range(1, len(train2)):
  dataset[i] = train2[i]

# This is picking the first element of data set that is for testing and storing it into a variable
row0 = dataset[0]

#making an array to fit all the distances
# The np.empty is to make sure the size matches the amount of data. it is just initializing it to zeros


# calculatig the distance between the first element of the data set array (which is the test case)
# To all the other elements which are train elements
for row in dataset:
  distance = get_distance2(row0, row)
  print(distance)

from sklearn.model_selection import train_test_split

# prameters for x axis
x = fruits['mass']
# prams for y axis
y = fruits['width']
# parameters for class identifier
label = fruits['fruit_label']

# Testing
#print(x, y, label)

# making test and train arrays using the method in sklearn package
# Randomly assign test cases and train cases. 
# Choosing the test size to be 30% of the data therefor train would be 70% of the dataset
x_train, x_test, y_train, y_test, label_train, label_test = train_test_split ( x , y , label, test_size = 0.3 ,random_state=0)

# Testing
#list(label_test)
#zip together the data for x and y

#Train is used for the algorithm to memorize
train2 = list(zip(x_train, y_train, label_train))

#Test is used for the algorithm to pick a point and test the algorithm itself 
#using the train set.
test2 = list(zip(x_test, y_test, label_test))

print(train2)
print(test2)
for index, tuple in enumerate(train2):
  x1 = tuple[0]
  y1 = tuple[1]
  label1 = tuple[2]
  print(x1, y1, label1)
for index, tuple in enumerate(test2):
  x2 = tuple[0]
  y2 = tuple[1]
  label2 = tuple[2]
  print(x2, y2, label2)

from sklearn.model_selection import train_test_split

# prameters for x axis
X = fruits[['mass', 'width','height']]
# prams for y axis
Y = fruits['fruit_label']

print(X)


# making test and train arrays using the method in sklearn package
# Randomly assign test cases and train cases. 
# Choosing the test size to be 30% of the data therefor train would be 70% of the dataset
X_train, X_test, y_train, y_test = train_test_split ( X , Y , test_size = 0.3 ,random_state=0)

#notice the indices they are randomly chosen.
print("\nthis is x train data randomly selected: ")
print(X_train)

#notice the indices they are randomly chosen.
print("\nthis is x test data randomly selected: ")
print(X_test)

#Blue points are training points
plt.scatter(X_train['mass'] , X_train['height'])
#orange points are testing points
plt.scatter(X_test['mass'] , X_test['height'])

#Ask user for the number of K
K = input("Enter the value of K: ")
print("K is: " + K)

#zip together the data for x and y

#Train is used for the algorithm to memorize
train = list(zip(X_train['mass'], y_train))

#Test is used for the algorithm to pick a point and test the algorithm itself 
#using the train set.
test = list(zip(X_test['mass'] , y_test))


print(train)
print(test)

get_distance([0,0],[3,4])
list(train)

get_distance(train,test)

print("This is x_train data: ")
#Mass , width , height
X_test.describe()

print("\nThis is y_train data: ")
#Just has labels
y_train.describe()


get_distance(train[0][0], train[0][1])

from collections import Counter
 
# given an array of nearest neighbours for a test case, 
# tally up their classes to vote on test case class
# basically count frequencies and get the mode of them
 
def get_majority_vote(neighbours):

# Step 1: Calculate the Euclidian distance between two rows in our dataset (two different data points)

def get_distance(data1, data2):
  """Caluclates the distance between two vectors 

    Parameters
    ----------
    data1 : vector
        A row from our data set
    data2 : vector
        A row from our data set

    Returns
    -------
    sqrt(distance)
        the euclidian distance between the two data points
    """
	for i in range(len(data1)-1):
		distance += (data1[i] - data2[i])**2    # Utilize formula for euclidian distance
	return sqrt(distance)
 
# Step 2: Get Nearest Neighbors by sorting the distances in a non-decreasing fashion and select the first k neighbors

def get_neighbors(train, test_data, k):
  """Locates the most similar neighbors by sorting by distances

    Parameters
    ----------
    train : list
        A list of rows containing pre-existing data to train algorithm 
    test_data : vector
        A row of data that needs to be classified
    k : int
        The number of desired neighbors

    Returns
    -------
    neighbors
        the k closest training data points
    """
	distances = list()                            # Initialize list of distances
	for train_data in train:                      
		dist = get_distance(test_data, train_data)  # Find distance between inputted test_data and every train_data in train
		distances.append((train_row, dist))         # Add this distance to our list of distances
	distances.sort(key=lambda tup: tup[1])        # Sort list of distances in non-decreasing fashion
	neighbors = list()                            # Initialize list of neigboring points to test_data
	for i in range(k):
		neighbors.append(distances[i][0])           # Add first k training data points according to their distances to list of neighboring points
	return neighbors                              # Return the k closest training data points (already classfified points) to the test data point 

# Step 3: Make predictions based on the k closest training data points from get_neigbors() function

def predict(train, test_data, k):
  """Makes a classification prediction using k nearest neighbors

    Parameters
    ----------
    train : list
        A list of rows containing pre-existing data to train algorithm 
    test_data : vector
        A row of data that needs to be classified
    k : int
        The number of desired neighbors

    Returns
    -------
    prediction
        the most represented class among the k nearest neighbors
    """
	neighbors = get_neighbors(train, test_data, k)
	classes = [data_point[-1] for data_point in neighbors]  # Sorted list of k closest training data points from get_neighbors()
	prediction = max(set(classes), key=classes.count)       # Take the class with the highest frequency
	return prediction                                       # Returns the most represented class among the k nearest neighbors